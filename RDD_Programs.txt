<<<<<<< HEAD
from pyspark import SparkContext
import os

os.environ["PYSPARK_PYTHON"]="C:\python\python.exe"

sc=SparkContext("local","RDDDemo")
data=[1,2,2,2,3]
rdd=sc.parallelize(data)
print(rdd.collect())

output:-
[1, 2, 2, 2, 3]



Problem 1:
from pyspark import SparkContext
sc = SparkContext("local", "WordCountWithFiltering")
sentences = [
    "This is a sample sentence",
    "The quick brown fox jumps over the lazy dog",
    "Spark is great for big data processing",
    "An example with the stopwords removed"
]
stopwords = {"is", "the", "a", "an", "for", "with", "over"}
rdd = sc.parallelize(sentences)
word_counts = (
    rdd
    .flatMap(lambda line: line.lower().split())                        
    .filter(lambda word: word not in stopwords)                        
    .map(lambda word: (word, 1))                                       
    .reduceByKey(lambda x, y: x + y)                                   
)
output = word_counts.collect()
for word, count in output:
    print(f"{word}: {count}")

print("\nTop 5 words:")
print(word_counts.take(5))
sc.stop()


OUTPUT:-

this: 1
sample: 1
sentence: 1
quick: 1
brown: 1
fox: 1
jumps: 1
lazy: 1
dog: 1
spark: 1
great: 1
big: 1
data: 1
processing: 1
example: 1
stopwords: 1
removed: 1

Top 5 words:
[('this', 1), ('sample', 1), ('sentence', 1), ('quick', 1), ('brown', 1)]






PROGRAM 2:-
from pyspark import SparkContext

sc = SparkContext("local", "AverageScore")

# Input Data
data = [("Alice", 80), ("Bob", 90), ("Alice", 70), ("Bob", 85), ("Charlie", 60)]

rdd = sc.parallelize(data)

# Map to (name, (score, 1))
score_pairs = rdd.map(lambda x: (x[0], (x[1], 1)))

#ReduceByKey to sum scores and counts
total_scores = score_pairs.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))

#Map to calculate average
average_scores = total_scores.map(lambda x: (x[0], x[1][0] / x[1][1]))

#Action - collect results
result = average_scores.collect()

#Sorted by name
sorted_result = average_scores.takeOrdered(10, key=lambda x: x[0])

for name, avg in sorted_result:
    print(f"{name}: {avg:.2f}")


OUTPUT:-
Alice: 75.00
Bob: 87.50
Charlie: 60.00




PROGRAM 3:-

from pyspark import SparkContext

#SparkContext initialization
sc = SparkContext.getOrCreate()

# Input List
numbers = [5, 3, 4, 5, 2, 3, 5, 3, 4]
rdd = sc.parallelize(numbers)

# Transformations
pairs = rdd.map(lambda x: (x, 1))
counts = pairs.reduceByKey(lambda a, b: a + b)
swapped = counts.map(lambda x: (x[1], x[0]))
sorted_counts = swapped.sortByKey(ascending=False)

# Action
top3 = sorted_counts.take(3)

# Output
for count, number in top3:
    print(f"Number: {number}, Frequency: {count}")


OUTPUT:-
Number: 5, Frequency: 3
Number: 3, Frequency: 3
Number: 4, Frequency: 2










=======
from pyspark import SparkContext
import os

os.environ["PYSPARK_PYTHON"]="C:\python\python.exe"

sc=SparkContext("local","RDDDemo")
data=[1,2,2,2,3]
rdd=sc.parallelize(data)
print(rdd.collect())

output:-
[1, 2, 2, 2, 3]



Problem 1:
from pyspark import SparkContext
sc = SparkContext("local", "WordCountWithFiltering")
sentences = [
    "This is a sample sentence",
    "The quick brown fox jumps over the lazy dog",
    "Spark is great for big data processing",
    "An example with the stopwords removed"
]
stopwords = {"is", "the", "a", "an", "for", "with", "over"}
rdd = sc.parallelize(sentences)
word_counts = (
    rdd
    .flatMap(lambda line: line.lower().split())                        
    .filter(lambda word: word not in stopwords)                        
    .map(lambda word: (word, 1))                                       
    .reduceByKey(lambda x, y: x + y)                                   
)
output = word_counts.collect()
for word, count in output:
    print(f"{word}: {count}")

print("\nTop 5 words:")
print(word_counts.take(5))
sc.stop()


OUTPUT:-

this: 1
sample: 1
sentence: 1
quick: 1
brown: 1
fox: 1
jumps: 1
lazy: 1
dog: 1
spark: 1
great: 1
big: 1
data: 1
processing: 1
example: 1
stopwords: 1
removed: 1

Top 5 words:
[('this', 1), ('sample', 1), ('sentence', 1), ('quick', 1), ('brown', 1)]






PROGRAM 2:-
from pyspark import SparkContext

sc = SparkContext("local", "AverageScore")

# Input Data
data = [("Alice", 80), ("Bob", 90), ("Alice", 70), ("Bob", 85), ("Charlie", 60)]

rdd = sc.parallelize(data)

# Map to (name, (score, 1))
score_pairs = rdd.map(lambda x: (x[0], (x[1], 1)))

#ReduceByKey to sum scores and counts
total_scores = score_pairs.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))

#Map to calculate average
average_scores = total_scores.map(lambda x: (x[0], x[1][0] / x[1][1]))

#Action - collect results
result = average_scores.collect()

#Sorted by name
sorted_result = average_scores.takeOrdered(10, key=lambda x: x[0])

for name, avg in sorted_result:
    print(f"{name}: {avg:.2f}")


OUTPUT:-
Alice: 75.00
Bob: 87.50
Charlie: 60.00




PROGRAM 3:-

from pyspark import SparkContext

#SparkContext initialization
sc = SparkContext.getOrCreate()

# Input List
numbers = [5, 3, 4, 5, 2, 3, 5, 3, 4]
rdd = sc.parallelize(numbers)

# Transformations
pairs = rdd.map(lambda x: (x, 1))
counts = pairs.reduceByKey(lambda a, b: a + b)
swapped = counts.map(lambda x: (x[1], x[0]))
sorted_counts = swapped.sortByKey(ascending=False)

# Action
top3 = sorted_counts.take(3)

# Output
for count, number in top3:
    print(f"Number: {number}, Frequency: {count}")


OUTPUT:-
Number: 5, Frequency: 3
Number: 3, Frequency: 3
Number: 4, Frequency: 2










>>>>>>> 573415f835dafab5f1bebd40edcafedf599283f6
